{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'positive', 'score': 0.7101832628250122}, {'label': 'negative', 'score': 0.17953164875507355}, {'label': 'neutral', 'score': 0.11028500646352768}]]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", top_k=None)\n",
    "\n",
    "# Test the model with a sentence\n",
    "result = sentiment_analyzer(\"improves GDP by 5% and increases homelessness by 5%\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_harris_sentiment.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_harris_sentiment.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_harris_sentiment.csv'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train_harris_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the 'candidate' and 'publishedAt' columns\n",
    "df_train_final = df_train.drop(columns=['candidate', 'publishedAt'])\n",
    "\n",
    "df_train_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db406c4a",
   "metadata": {},
   "source": [
    "# Save the modified dataset (Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_final.to_csv(\"harris_train_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all words in articles have tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m missing_tokens\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Loop through all articles in the dataframe\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df_train_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_train_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: check_tokens_in_vocab(x, tokenizer))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Check the missing tokens for each article\u001b[39;00m\n\u001b[0;32m     18\u001b[0m missing_tokens \u001b[38;5;241m=\u001b[39m df_train_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mexplode()\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39munique()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train_final' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Print out words not found in tokenizer\n",
    "def check_tokens_in_vocab(text, tokenizer):\n",
    "    words = text.split()  # Split the article into words\n",
    "    missing_tokens = []\n",
    "    for word in words:\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        if len(tokenized_word) == 0:  # If the tokenizer returns an empty list, the word is not tokenized\n",
    "            print(f\"Word not in tokenizer: {word}\")\n",
    "            missing_tokens.append(word)\n",
    "    return missing_tokens\n",
    "\n",
    "# Loop through all articles in the dataframe\n",
    "df_train_final['missing_tokens'] = df_train_final['article'].apply(lambda x: check_tokens_in_vocab(x, tokenizer))\n",
    "\n",
    "# Check the missing tokens for each article\n",
    "missing_tokens = df_train_final['missing_tokens'].explode().dropna().unique()\n",
    "print(f\"Unique missing tokens: {missing_tokens}\")\n",
    "\n",
    "# Optionally, display the missing tokens for review\n",
    "print(f\"Total unique missing tokens: {len(missing_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Check if some important words from word cloud are tokens if not add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized version of 'tim walz': ['tim', 'wa', '##lz']\n",
      "Tokenized version of 'lie': ['lie']\n",
      "Tokenized version of 'abortion': ['ab', '##ortion']\n",
      "Tokenized version of 'news': ['news']\n",
      "Tokenized version of 'post': ['post']\n",
      "Tokenized version of 'debate': ['debate']\n",
      "Tokenized version of 'vp': ['v', '##p']\n",
      "Tokenized version of 'cnn': ['cn', '##n']\n",
      "Tokenized version of 'rally': ['rally']\n",
      "Tokenized version of 'attack': ['attack']\n",
      "Tokenized version of 'told': ['told']\n",
      "Tokenized version of 'vice president': ['vice', 'president']\n",
      "Tokenized version of 'campaign': ['campaign']\n",
      "Tokenized version of 'appeared': ['appeared']\n",
      "Tokenized version of 'voter': ['voter']\n",
      "Tokenized version of 'economy': ['economy']\n",
      "Tokenized version of 'rant': ['ran', '##t']\n",
      "Tokenized version of 'republican': ['republic', '##an']\n",
      "Tokenized version of 'pennsylvania': ['penn', '##sy', '##lva', '##nia']\n",
      "Tokenized version of 'democratic': ['democratic']\n",
      "Tokenized version of 'vote': ['vote']\n",
      "Tokenized version of 'speech': ['speech']\n",
      "Tokenized version of 'right': ['right']\n",
      "Tokenized version of 'georgia': ['ge', '##org', '##ia']\n",
      "Tokenized version of 'support': ['support']\n",
      "Tokenized version of 'policy': ['policy']\n",
      "Tokenized version of 'liz cheney': ['li', '##z', 'che', '##ney']\n",
      "Tokenized version of 'endorsed': ['endorsed']\n",
      "Tokenized version of 'gop': ['go', '##p']\n",
      "Tokenized version of 'endorsement': ['end', '##orse', '##ment']\n",
      "Tokenized version of 'country': ['country']\n",
      "Tokenized version of 'video': ['video']\n",
      "Tokenized version of 'abc': ['ab', '##c']\n",
      "Tokenized version of 'host': ['host']\n",
      "Tokenized version of 'among': ['among']\n",
      "Tokenized version of 'joe': ['jo', '##e']\n",
      "Tokenized version of 'democrat': ['demo', '##crat']\n",
      "Tokenized version of 'former': ['former']\n",
      "Tokenized version of 'try': ['try']\n",
      "Tokenized version of 'claimed': ['claimed']\n",
      "Tokenized version of 'assassination': ['assassination']\n",
      "Tokenized version of 'political': ['political']\n",
      "Tokenized version of 'people': ['people']\n",
      "Tokenized version of 'administration': ['administration']\n",
      "Tokenized version of 'fox': ['f', '##ox']\n",
      "Tokenized version of 'voting': ['voting']\n",
      "Tokenized version of 'white': ['white']\n",
      "Tokenized version of 'win': ['win']\n",
      "Tokenized version of 'american': ['american']\n",
      "Tokenized version of 'health': ['health']\n",
      "Tokenized version of 'biden': ['bid', '##en']\n",
      "Tokenized version of 'record': ['record']\n",
      "Tokenized version of 'poll': ['poll']\n",
      "Tokenized version of 'house': ['house']\n",
      "Tokenized version of 'woman': ['woman']\n",
      "Tokenized version of 'lead': ['lead']\n",
      "Tokenized version of 'candidate': ['candidate']\n",
      "Tokenized version of 'maga': ['maga']\n",
      "Tokenized version of 'state': ['state']\n",
      "Tokenized version of 'wisconsin': ['wis', '##cons', '##in']\n",
      "Tokenized version of 'election': ['election']\n",
      "Tokenized version of 'black': ['black']\n",
      "Tokenized version of 'nominee': ['nominee']\n",
      "Tokenized version of 'report': ['report']\n",
      "Tokenized version of 'year': ['year']\n",
      "Tokenized version of 'world': ['world']\n",
      "Tokenized version of 'live': ['live']\n",
      "Tokenized version of 'endorses': ['end', '##orse', '##s']\n",
      "Tokenized version of 'presidential': ['presidential']\n",
      "Tokenized version of 'plan': ['plan']\n",
      "Tokenized version of 'said': ['said']\n",
      "Tokenized version of 'america': ['am', '##eri', '##ca']\n",
      "Tokenized version of 'would': ['would']\n",
      "Tokenized version of 'back': ['back']\n",
      "Tokenized version of 'supporter': ['supporter']\n",
      "Tokenized version of 'border': ['border']\n",
      "Tokenized version of 'view': ['view']\n",
      "Tokenized version of 'charlamagne': ['char', '##lama', '##gne']\n",
      "Tokenized version of 'hurricane': ['hurricane']\n",
      "Tokenized version of 'podcast': ['pod', '##cast']\n",
      "Tokenized version of 'interview': ['interview']\n",
      "Tokenized version of 'msnbc': ['ms', '##n', '##bc']\n",
      "Tokenized version of 'elon musk': ['el', '##on', 'mu', '##sk']\n",
      "Tokenized version of 'medical': ['medical']\n",
      "Tokenized version of 'town hall': ['town', 'hall']\n",
      "Tokenized version of 'bret baier': ['br', '##et', 'baie', '##r']\n",
      "Tokenized version of 'protrump': ['pro', '##trum', '##p']\n",
      "Tokenized version of 'carolina': ['car', '##olina']\n"
     ]
    }
   ],
   "source": [
    "words = ['tim walz', 'lie', 'abortion', 'news', 'post', 'debate', \n",
    "        'vp', 'cnn', 'rally', 'attack', 'told', 'vice president', 'campaign',\n",
    "         'appeared', 'voter', 'economy', 'rant', \n",
    "         'republican', 'pennsylvania', 'democratic', 'vote', 'speech', 'right', 'georgia', 'support', \n",
    "         'policy', 'liz cheney', 'endorsed', 'gop', 'endorsement', 'country', 'video', \n",
    "         'abc', 'host', 'among', 'joe', 'democrat', 'former', 'try', 'claimed', 'assassination', \n",
    "         'political', 'people', 'administration', 'fox',\n",
    "        'voting', 'white', 'win', 'american', 'health', 'biden',\n",
    "        'record', 'poll', 'house', 'woman', 'lead', 'candidate', \n",
    "         'maga', 'state', 'wisconsin', 'election', 'black', \n",
    "         'nominee', 'report', 'year', 'world', 'live', 'endorses', \n",
    "         'presidential', 'plan', 'said', 'america', 'would', 'back', 'supporter', 'border', 'view', 'charlamagne', \n",
    "         'hurricane', 'podcast', 'interview', 'msnbc', 'elon musk', 'medical', 'town hall', \n",
    "         'bret baier', 'protrump', 'carolina']\n",
    "\n",
    "\n",
    "\n",
    "# Function to check if each word in the array is tokenized\n",
    "def check_multiple_words_tokenized(words_list, tokenizer):\n",
    "    for word in words_list:\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        print(f\"Tokenized version of '{word}': {tokenized_word}\")\n",
    "        \n",
    "check_multiple_words_tokenized(words, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 28 new tokens: ['tim walz', 'abortion', 'vp', 'cnn', 'vice president', 'rant', 'republican', 'pennsylvania', 'georgia', 'liz cheney', 'gop', 'endorsement', 'abc', 'joe', 'democrat', 'fox', 'biden', 'wisconsin', 'endorses', 'america', 'charlamagne', 'podcast', 'msnbc', 'elon musk', 'town hall', 'bret baier', 'protrump', 'carolina']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(119575, 768, padding_idx=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which words need to be added as tokens\n",
    "missing_tokens = []\n",
    "for word in words:\n",
    "    tokenized_word = tokenizer.tokenize(word)\n",
    "    if len(tokenized_word) > 1 or any(t.startswith(\"##\") for t in tokenized_word):\n",
    "        missing_tokens.append(word)\n",
    "\n",
    "# Add missing tokens to the tokenizer\n",
    "if missing_tokens:\n",
    "    tokenizer.add_tokens(missing_tokens)\n",
    "    print(f\"Added {len(missing_tokens)} new tokens: {missing_tokens}\")\n",
    "else:\n",
    "    print(\"No new tokens needed.\")\n",
    "\n",
    "# Resize the modelâ€™s token embeddings to match the new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings(\n",
       "  (word_embeddings): Embedding(119575, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distilbert.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6ba80",
   "metadata": {},
   "source": [
    "# Save pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens_kamala\\\\tokenizer_config.json',\n",
       " 'C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens_kamala\\\\special_tokens_map.json',\n",
       " 'C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens_kamala\\\\vocab.txt',\n",
       " 'C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens_kamala\\\\added_tokens.json',\n",
       " 'C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens_kamala\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens_kamala\")\n",
    "tokenizer.save_pretrained(\"C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens_kamala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea5418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
