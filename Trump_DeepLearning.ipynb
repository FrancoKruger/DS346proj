{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'positive', 'score': 0.7101832628250122}, {'label': 'negative', 'score': 0.17953164875507355}, {'label': 'neutral', 'score': 0.11028500646352768}]]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", top_k=None)\n",
    "\n",
    "# Test the model with a sentence\n",
    "result = sentiment_analyzer(\"improves GDP by 5% and increases homelessness by 5%\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train_trump_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>candidate</th>\n",
       "      <th>score</th>\n",
       "      <th>publishedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gutwrenching ad show horror abortion ban new w...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>-0.9274</td>\n",
       "      <td>2024-09-19T13:00:01Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campaign influence operation say former state ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>2024-10-11T15:19:22Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new powerful ad going break powerful new ad ha...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>2024-10-07T22:02:33Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surrounded disaster mayhem entering final week...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>-0.9517</td>\n",
       "      <td>2024-10-03T00:14:05Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agrees charlamagne tha god assessment trump ca...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>2024-10-16T01:37:10Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article     candidate   score  \\\n",
       "0  gutwrenching ad show horror abortion ban new w...  Donald Trump -0.9274   \n",
       "1  campaign influence operation say former state ...  Donald Trump  0.2960   \n",
       "2  new powerful ad going break powerful new ad ha...  Donald Trump  0.5267   \n",
       "3  surrounded disaster mayhem entering final week...  Donald Trump -0.9517   \n",
       "4  agrees charlamagne tha god assessment trump ca...  Donald Trump  0.4404   \n",
       "\n",
       "            publishedAt  \n",
       "0  2024-09-19T13:00:01Z  \n",
       "1  2024-10-11T15:19:22Z  \n",
       "2  2024-10-07T22:02:33Z  \n",
       "3  2024-10-03T00:14:05Z  \n",
       "4  2024-10-16T01:37:10Z  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gutwrenching ad show horror abortion ban new w...</td>\n",
       "      <td>-0.9274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campaign influence operation say former state ...</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new powerful ad going break powerful new ad ha...</td>\n",
       "      <td>0.5267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surrounded disaster mayhem entering final week...</td>\n",
       "      <td>-0.9517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agrees charlamagne tha god assessment trump ca...</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article   score\n",
       "0  gutwrenching ad show horror abortion ban new w... -0.9274\n",
       "1  campaign influence operation say former state ...  0.2960\n",
       "2  new powerful ad going break powerful new ad ha...  0.5267\n",
       "3  surrounded disaster mayhem entering final week... -0.9517\n",
       "4  agrees charlamagne tha god assessment trump ca...  0.4404"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'candidate' and 'publishedAt' columns\n",
    "df_train_final = df_train.drop(columns=['candidate', 'publishedAt'])\n",
    "\n",
    "df_train_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754b961",
   "metadata": {},
   "source": [
    "# Save the modified dataset (Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "503e8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_final.to_csv(\"trump_train_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all words in articles have tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique missing tokens: []\n",
      "Total unique missing tokens: 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Print out words not found in tokenizer\n",
    "def check_tokens_in_vocab(text, tokenizer):\n",
    "    words = text.split()  # Split the article into words\n",
    "    missing_tokens = []\n",
    "    for word in words:\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        if len(tokenized_word) == 0:  # If the tokenizer returns an empty list, the word is not tokenized\n",
    "            print(f\"Word not in tokenizer: {word}\")\n",
    "            missing_tokens.append(word)\n",
    "    return missing_tokens\n",
    "\n",
    "# Loop through all articles in the dataframe\n",
    "df_train_final['missing_tokens'] = df_train_final['article'].apply(lambda x: check_tokens_in_vocab(x, tokenizer))\n",
    "\n",
    "# Check the missing tokens for each article\n",
    "missing_tokens = df_train_final['missing_tokens'].explode().dropna().unique()\n",
    "print(f\"Unique missing tokens: {missing_tokens}\")\n",
    "\n",
    "# Optionally, display the missing tokens for review\n",
    "print(f\"Total unique missing tokens: {len(missing_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Check if some important words from word cloud are tokens if not add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized version of 'candidate': ['candidate']\n",
      "Tokenized version of 'battleground': ['battle', '##ground']\n",
      "Tokenized version of 'state': ['state']\n",
      "Tokenized version of 'presidential': ['presidential']\n",
      "Tokenized version of 'new york': ['new', 'yo', '##rk']\n",
      "Tokenized version of 'cnn': ['cn', '##n']\n",
      "Tokenized version of 'mark robinson': ['mark', 'ro', '##bin', '##son']\n",
      "Tokenized version of 'report': ['report']\n",
      "Tokenized version of 'jd vance': ['j', '##d', 'van', '##ce']\n",
      "Tokenized version of 'democratic': ['democratic']\n",
      "Tokenized version of 'post': ['post']\n",
      "Tokenized version of 'win': ['win']\n",
      "Tokenized version of 'news': ['news']\n",
      "Tokenized version of 'poll': ['poll']\n",
      "Tokenized version of 'among': ['among']\n",
      "Tokenized version of 'voter': ['voter']\n",
      "Tokenized version of 'polling': ['poll', '##ing']\n",
      "Tokenized version of 'support': ['support']\n",
      "Tokenized version of 'former': ['former']\n",
      "Tokenized version of 'president': ['president']\n",
      "Tokenized version of 'election': ['election']\n",
      "Tokenized version of 'georgia': ['ge', '##org', '##ia']\n",
      "Tokenized version of 'vote': ['vote']\n",
      "Tokenized version of 'race': ['race']\n",
      "Tokenized version of 'democrat': ['demo', '##crat']\n",
      "Tokenized version of 'texas': ['te', '##xas']\n",
      "Tokenized version of 'jury': ['jury']\n",
      "Tokenized version of 'bidenharris': ['bid', '##en', '##har', '##ris']\n",
      "Tokenized version of 'federal': ['federal']\n",
      "Tokenized version of 'republican': ['republic', '##an']\n",
      "Tokenized version of 'vice': ['vice']\n",
      "Tokenized version of 'united': ['united']\n",
      "Tokenized version of 'nominee': ['nominee']\n",
      "Tokenized version of 'white': ['white']\n",
      "Tokenized version of 'biden': ['bid', '##en']\n",
      "Tokenized version of 'rhetoric': ['r', '##het', '##ori', '##c']\n",
      "Tokenized version of 'assassination': ['assassination']\n",
      "Tokenized version of 'administration': ['administration']\n",
      "Tokenized version of 'interview': ['interview']\n",
      "Tokenized version of 'debate': ['debate']\n",
      "Tokenized version of 'campaign': ['campaign']\n",
      "Tokenized version of 'gop': ['go', '##p']\n",
      "Tokenized version of 'house': ['house']\n",
      "Tokenized version of 'supporter': ['supporter']\n",
      "Tokenized version of 'called': ['called']\n",
      "Tokenized version of 'claim': ['claim']\n",
      "Tokenized version of 'analyst': ['anal', '##yst']\n",
      "Tokenized version of 'rally': ['rally']\n",
      "Tokenized version of 'lie': ['lie']\n",
      "Tokenized version of 'call': ['call']\n",
      "Tokenized version of 'lead': ['lead']\n",
      "Tokenized version of 'vp': ['v', '##p']\n",
      "Tokenized version of 'key': ['key']\n",
      "Tokenized version of 'show': ['show']\n",
      "Tokenized version of 'town hall': ['town', 'hall']\n",
      "Tokenized version of 'fox': ['f', '##ox']\n",
      "Tokenized version of 'policy': ['policy']\n",
      "Tokenized version of 'tim walz': ['tim', 'wa', '##lz']\n",
      "Tokenized version of 'attack': ['attack']\n",
      "Tokenized version of 'american': ['american']\n",
      "Tokenized version of 'tariff': ['tar', '##iff']\n",
      "Tokenized version of 'country': ['country']\n",
      "Tokenized version of 'helene': ['hele', '##ne']\n",
      "Tokenized version of 'host': ['host']\n",
      "Tokenized version of 'swing': ['swing']\n",
      "Tokenized version of 'pennsylvania': ['penn', '##sy', '##lva', '##nia']\n",
      "Tokenized version of 'joe': ['jo', '##e']\n",
      "Tokenized version of 'hurricane': ['hurricane']\n",
      "Tokenized version of 'woman': ['woman']\n",
      "Tokenized version of 'political': ['political']\n",
      "Tokenized version of 'obama': ['oba', '##ma']\n",
      "Tokenized version of 'economy': ['economy']\n",
      "Tokenized version of 'issue': ['issue']\n",
      "Tokenized version of 'plan': ['plan']\n",
      "Tokenized version of 'record': ['record']\n",
      "Tokenized version of 'elon musk': ['el', '##on', 'mu', '##sk']\n",
      "Tokenized version of 'endorse': ['end', '##orse']\n",
      "Tokenized version of 'washington': ['was', '##hing', '##ton']\n",
      "Tokenized version of 'israel': ['is', '##rae', '##l']\n",
      "Tokenized version of 'carolina': ['car', '##olina']\n",
      "Tokenized version of 'abc': ['ab', '##c']\n"
     ]
    }
   ],
   "source": [
    "words = ['candidate', 'battleground', 'state', 'presidential', 'new york',\n",
    "        'cnn', 'mark robinson', 'report', 'jd vance',\n",
    "        'democratic', 'post', 'win', 'news', \n",
    "        'poll', 'among', 'voter', 'polling', 'support', 'former', 'president',\n",
    "        'election', 'georgia', 'vote', 'race', 'democrat', 'texas', \n",
    "        'jury', 'bidenharris', 'federal', 'republican', \n",
    "        'vice', 'united', 'nominee', 'white', 'biden',\n",
    "        'rhetoric', 'assassination',\n",
    "        'administration', 'interview', 'debate',\n",
    "        'campaign','gop', 'house', 'supporter', \n",
    "        'called', 'claim', 'analyst', 'rally', 'lie', 'call',\n",
    "        'lead', 'vp', 'key', 'show', 'town hall', 'fox', 'policy', \n",
    "        'tim walz', 'attack', 'american', 'tariff', 'country', 'helene', 'host',\n",
    "        'swing', 'pennsylvania', 'joe', 'hurricane', 'woman', 'political','obama', \n",
    "        'economy', 'issue', 'plan', 'record', 'elon musk', 'endorse', 'washington',\n",
    "        'israel', 'carolina', 'abc']\n",
    "\n",
    "# Function to check if each word in the array is tokenized\n",
    "def check_multiple_words_tokenized(words_list, tokenizer):\n",
    "    for word in words_list:\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        print(f\"Tokenized version of '{word}': {tokenized_word}\")\n",
    "        \n",
    "check_multiple_words_tokenized(words, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Add tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 30 new tokens: ['battleground', 'new york', 'cnn', 'mark robinson', 'jd vance', 'polling', 'georgia', 'democrat', 'texas', 'bidenharris', 'republican', 'biden', 'rhetoric', 'gop', 'analyst', 'vp', 'town hall', 'fox', 'tim walz', 'tariff', 'helene', 'pennsylvania', 'joe', 'obama', 'elon musk', 'endorse', 'washington', 'israel', 'carolina', 'abc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(119577, 768, padding_idx=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which words need to be added as tokens\n",
    "missing_tokens = []\n",
    "for word in words:\n",
    "    tokenized_word = tokenizer.tokenize(word)\n",
    "    if len(tokenized_word) > 1 or any(t.startswith(\"##\") for t in tokenized_word):\n",
    "        missing_tokens.append(word)\n",
    "\n",
    "# Add missing tokens to the tokenizer\n",
    "if missing_tokens:\n",
    "    tokenizer.add_tokens(missing_tokens)\n",
    "    print(f\"Added {len(missing_tokens)} new tokens: {missing_tokens}\")\n",
    "else:\n",
    "    print(\"No new tokens needed.\")\n",
    "\n",
    "# Resize the model’s token embeddings to match the new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings(\n",
       "  (word_embeddings): Embedding(119577, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distilbert.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1365f1",
   "metadata": {},
   "source": [
    "# Save pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "995f44d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens\\\\tokenizer_config.json',\n",
       " 'C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens\\\\special_tokens_map.json',\n",
       " 'C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens\\\\vocab.txt',\n",
       " 'C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens\\\\added_tokens.json',\n",
       " 'C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens\")\n",
    "tokenizer.save_pretrained(\"C:/Users/User/Documents/3rd year/Datsci346/Project/DS346proj/model_with_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f929c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
