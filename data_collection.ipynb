{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Cleaning News Articles - Using NewsAPI\n",
    "\n",
    "In this notebook, we collect news articles related to **Donald Trump** and **Kamala Harris** using the NewsAPI, apply data cleaning techniques, and prepare the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY = \"9c56272ec3ef4a73a5bfe06c8bc1a4e9\"\n",
    "URL = \"https://newsapi.org/v2/everything\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching News Articles from NewsAPI\n",
    "\n",
    "We use the **NewsAPI** to gather news articles that mention **Donald Trump** and **Kamala Harris**. Our goal is to fetch articles where:\n",
    "\n",
    "- **Trump** is mentioned in the title or description but **Kamala Harris** is not.\n",
    "- **Kamala Harris** is mentioned in the title or description but **Donald Trump** is not.\n",
    "\n",
    "To achieve this, we split the search into 5 weekly intervals between **2024-09-17** and **2024-10-22**. For each interval, we fetch relevant articles and save them in two separate JSON files:\n",
    "- **trump_articles.json** for articles related to Donald Trump.\n",
    "- **harris_articles.json** for articles related to Kamala Harris.\n",
    "\n",
    "This ensures a broad coverage of articles while avoiding API rate limits and ensuring the relevance of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_articles(query, exclude, from_date, to_date, page_size=100):\n",
    "    params = {\n",
    "        \"q\": f\"{query} -{exclude}\",  # Exclude the other candidate\n",
    "        \"from\": from_date,\n",
    "        \"to\": to_date,\n",
    "        \"language\": \"en\",\n",
    "        \"pageSize\": page_size,\n",
    "        \"searchIn\": \"title,description\",\n",
    "        \"apiKey\": API_KEY,\n",
    "    }\n",
    "    response = requests.get(URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if data[\"status\"] != \"ok\":\n",
    "        print(f\"Error: {data['message']}\")\n",
    "        return []\n",
    "\n",
    "    return data.get(\"articles\", [])\n",
    "\n",
    "\n",
    "def save_articles_to_json(articles, filename):\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(articles, file, indent=4)\n",
    "\n",
    "\n",
    "# Function to split date range into weekly intervals\n",
    "def date_range_splitter(start_date, end_date, delta=7):\n",
    "    date_ranges = []\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=delta)\n",
    "        date_ranges.append(\n",
    "            (current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\"))\n",
    "        )\n",
    "        current_date = next_date\n",
    "    return date_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Trump articles from 2024-09-17 to 2024-09-24\n",
      "Fetching Harris articles from 2024-09-17 to 2024-09-24\n",
      "Fetching Trump articles from 2024-09-24 to 2024-10-01\n",
      "Fetching Harris articles from 2024-09-24 to 2024-10-01\n",
      "Fetching Trump articles from 2024-10-01 to 2024-10-08\n",
      "Fetching Harris articles from 2024-10-01 to 2024-10-08\n",
      "Fetching Trump articles from 2024-10-08 to 2024-10-15\n",
      "Fetching Harris articles from 2024-10-08 to 2024-10-15\n",
      "Fetching Trump articles from 2024-10-15 to 2024-10-22\n",
      "Fetching Harris articles from 2024-10-15 to 2024-10-22\n",
      "Saved 441 Trump articles and 500 Harris articles.\n"
     ]
    }
   ],
   "source": [
    "today = datetime.now()\n",
    "start_date = datetime(2024, 9, 17)\n",
    "date_ranges = date_range_splitter(\n",
    "    start_date, today, delta=7\n",
    ")  # Split into weekly intervals\n",
    "\n",
    "all_trump_articles = []\n",
    "all_harris_articles = []\n",
    "\n",
    "for from_date, to_date in date_ranges:\n",
    "    print(f\"Fetching Trump articles from {from_date} to {to_date}\")\n",
    "    trump_articles = fetch_articles(\"Donald Trump\", \"Kamala Harris\", from_date, to_date)\n",
    "    all_trump_articles.extend(trump_articles)\n",
    "\n",
    "    print(f\"Fetching Harris articles from {from_date} to {to_date}\")\n",
    "    harris_articles = fetch_articles(\"Kamala Harris\", \"Donald Trump\", from_date, to_date)\n",
    "    all_harris_articles.extend(harris_articles)\n",
    "\n",
    "save_articles_to_json(all_trump_articles, \"dataset/trump_articles.json\")\n",
    "save_articles_to_json(all_harris_articles, \"dataset/harris_articles.json\")\n",
    "\n",
    "print(\n",
    "    f\"Saved {len(all_trump_articles)} Trump articles and {len(all_harris_articles)} Harris articles.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "\n",
    "Once the data is collected, we proceed with cleaning to ensure consistency and remove irrelevant characters or noise. The following cleaning steps are applied to both the **title** and **description** fields:\n",
    "\n",
    "1. **Lowercasing**: Convert all characters to lowercase for uniformity.\n",
    "2. **URL Removal**: Remove any URLs from the text, as they do not contribute to the content analysis.\n",
    "3. **Unicode Character Removal**: Remove non-ASCII characters (e.g., `\\u00a0`), which often appear as encoding artifacts.\n",
    "4. **Non-Alphanumeric Character Removal**: Remove all non-alphanumeric characters, except spaces, to focus on meaningful words.\n",
    "5. **Whitespace Normalization**: Replace multiple spaces with a single space to ensure clean formatting.\n",
    "6. **Newline and Tab Removal**: Remove any newline or tab characters to keep the text on a single line.\n",
    "\n",
    "Any **Removed** or **duplicate** articles are also excluded from the dataset.\n",
    "\n",
    "After applying these steps, we save the cleaned data into new JSON files:\n",
    "- **cleaned_trump_articles.json**\n",
    "- **cleaned_harris_articles.json**\n",
    "\n",
    "This cleaned data will be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/trump_articles.json\", \"r\") as file:\n",
    "    trump_articles = json.load(file)\n",
    "\n",
    "with open(\"dataset/harris_articles.json\", \"r\") as file:\n",
    "    harris_articles = json.load(file)\n",
    "\n",
    "df_trump = pd.DataFrame(trump_articles)\n",
    "df_harris = pd.DataFrame(harris_articles)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"[\\n\\t]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to filter out entries with \"removed\" in title or description\n",
    "def filter_removed_entries(df):\n",
    "    # Remove rows where title or description contains 'removed'\n",
    "    df_filtered = df[~((df['title'].str.contains('removed', case=False, na=False)) |\n",
    "                       (df['description'].str.contains('removed', case=False, na=False)))]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data: 361 Trump articles and 407 Harris articles saved.\n"
     ]
    }
   ],
   "source": [
    "# Apply enhanced cleaning to title and description columns\n",
    "df_trump['title'] = df_trump['title'].apply(lambda x: clean_text(x) if pd.notnull(x) else '')\n",
    "df_trump['description'] = df_trump['description'].apply(lambda x: clean_text(x) if pd.notnull(x) else '')\n",
    "\n",
    "df_harris['title'] = df_harris['title'].apply(lambda x: clean_text(x) if pd.notnull(x) else '')\n",
    "df_harris['description'] = df_harris['description'].apply(lambda x: clean_text(x) if pd.notnull(x) else '')\n",
    "\n",
    "# Remove duplicates (if any)\n",
    "df_trump = df_trump.drop_duplicates(subset=['title', 'description'])\n",
    "df_harris = df_harris.drop_duplicates(subset=['title', 'description'])\n",
    "\n",
    "# Filter out \"removed\" entries\n",
    "df_trump = filter_removed_entries(df_trump)\n",
    "df_harris = filter_removed_entries(df_harris)\n",
    "\n",
    "# Save cleaned data to new JSON files\n",
    "df_trump.to_json('dataset/cleaned_trump_articles.json', orient='records', indent=4)\n",
    "df_harris.to_json('dataset/cleaned_harris_articles.json', orient='records', indent=4)\n",
    "\n",
    "print(f\"Cleaned data: {len(df_trump)} Trump articles and {len(df_harris)} Harris articles saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
