{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY = \"9c56272ec3ef4a73a5bfe06c8bc1a4e9\"\n",
    "URL = \"https://newsapi.org/v2/everything\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_articles(query, exclude, from_date, to_date, page_size=100):\n",
    "    params = {\n",
    "        \"q\": f\"{query} -{exclude}\",  # Exclude the other candidate\n",
    "        \"from\": from_date,\n",
    "        \"to\": to_date,\n",
    "        \"language\": \"en\",\n",
    "        \"pageSize\": page_size,\n",
    "        \"searchIn\": \"title,description\",  # Limit search to title and description\n",
    "        \"apiKey\": API_KEY,\n",
    "    }\n",
    "    response = requests.get(URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if data[\"status\"] != \"ok\":\n",
    "        print(f\"Error: {data['message']}\")\n",
    "        return []\n",
    "\n",
    "    return data.get(\"articles\", [])\n",
    "\n",
    "\n",
    "def save_articles_to_json(articles, filename):\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(articles, file, indent=4)\n",
    "\n",
    "\n",
    "# Function to split date range into weekly intervals\n",
    "def date_range_splitter(start_date, end_date, delta=7):\n",
    "    date_ranges = []\n",
    "    current_date = start_date\n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=delta)\n",
    "        date_ranges.append(\n",
    "            (current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\"))\n",
    "        )\n",
    "        current_date = next_date\n",
    "    return date_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Trump articles from 2024-09-17 to 2024-09-24\n",
      "Fetching Harris articles from 2024-09-17 to 2024-09-24\n",
      "Fetching Trump articles from 2024-09-24 to 2024-10-01\n",
      "Fetching Harris articles from 2024-09-24 to 2024-10-01\n",
      "Fetching Trump articles from 2024-10-01 to 2024-10-08\n",
      "Fetching Harris articles from 2024-10-01 to 2024-10-08\n",
      "Fetching Trump articles from 2024-10-08 to 2024-10-15\n",
      "Fetching Harris articles from 2024-10-08 to 2024-10-15\n",
      "Fetching Trump articles from 2024-10-15 to 2024-10-22\n",
      "Fetching Harris articles from 2024-10-15 to 2024-10-22\n",
      "Saved 441 Trump articles and 500 Harris articles.\n"
     ]
    }
   ],
   "source": [
    "# Set the date range for the last month\n",
    "today = datetime.now()\n",
    "start_date = datetime(2024, 9, 17)\n",
    "date_ranges = date_range_splitter(\n",
    "    start_date, today, delta=7\n",
    ")  # Split into weekly intervals\n",
    "\n",
    "all_trump_articles = []\n",
    "all_harris_articles = []\n",
    "\n",
    "# Fetch articles for each date range\n",
    "for from_date, to_date in date_ranges:\n",
    "    print(f\"Fetching Trump articles from {from_date} to {to_date}\")\n",
    "    trump_articles = fetch_articles(\"Donald Trump\", \"Kamala Harris\", from_date, to_date)\n",
    "    all_trump_articles.extend(trump_articles)\n",
    "\n",
    "    print(f\"Fetching Harris articles from {from_date} to {to_date}\")\n",
    "    harris_articles = fetch_articles(\"Kamala Harris\", \"Donald Trump\", from_date, to_date)\n",
    "    all_harris_articles.extend(harris_articles)\n",
    "\n",
    "# Save the articles in dataset/ folder\n",
    "save_articles_to_json(all_trump_articles, \"dataset/trump_articles.json\")\n",
    "save_articles_to_json(all_harris_articles, \"dataset/harris_articles.json\")\n",
    "\n",
    "print(\n",
    "    f\"Saved {len(all_trump_articles)} Trump articles and {len(all_harris_articles)} Harris articles.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved articles\n",
    "with open(\"dataset/trump_articles.json\", \"r\") as file:\n",
    "    trump_articles = json.load(file)\n",
    "\n",
    "with open(\"dataset/harris_articles.json\", \"r\") as file:\n",
    "    harris_articles = json.load(file)\n",
    "\n",
    "# Convert to DataFrame for easy manipulation\n",
    "df_trump = pd.DataFrame(trump_articles)\n",
    "df_harris = pd.DataFrame(harris_articles)\n",
    "\n",
    "# Basic cleaning functions\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove unicode characters (like \\u00a0)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()  # Ignore non-ASCII characters\n",
    "    # Remove non-alphanumeric characters (except spaces)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    # Remove newline and tab characters\n",
    "    text = re.sub(r\"[\\n\\t]\", \" \", text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to filter out entries with \"removed\" in title or description\n",
    "def filter_removed_entries(df):\n",
    "    # Remove rows where title or description contains 'removed'\n",
    "    df_filtered = df[~((df['title'].str.contains('removed', case=False, na=False)) |\n",
    "                       (df['description'].str.contains('removed', case=False, na=False)))]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data: 361 Trump articles and 407 Harris articles saved.\n"
     ]
    }
   ],
   "source": [
    "# Apply enhanced cleaning to title and description columns\n",
    "df_trump['title'] = df_trump['title'].apply(lambda x: clean_text(x) if pd.notnull(x) else '')\n",
    "df_trump['description'] = df_trump['description'].apply(lambda x: clean_text(x) if pd.notnull(x) else '')\n",
    "\n",
    "df_harris['title'] = df_harris['title'].apply(lambda x: clean_text(x) if pd.notnull(x) else '')\n",
    "df_harris['description'] = df_harris['description'].apply(lambda x: clean_text(x) if pd.notnull(x) else '')\n",
    "\n",
    "# Remove duplicates (if any)\n",
    "df_trump = df_trump.drop_duplicates(subset=['title', 'description'])\n",
    "df_harris = df_harris.drop_duplicates(subset=['title', 'description'])\n",
    "\n",
    "# Filter out \"removed\" entries\n",
    "df_trump = filter_removed_entries(df_trump)\n",
    "df_harris = filter_removed_entries(df_harris)\n",
    "\n",
    "# Save cleaned data to new JSON files\n",
    "df_trump.to_json('dataset/cleaned_trump_articles.json', orient='records', indent=4)\n",
    "df_harris.to_json('dataset/cleaned_harris_articles.json', orient='records', indent=4)\n",
    "\n",
    "print(f\"Cleaned data: {len(df_trump)} Trump articles and {len(df_harris)} Harris articles saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
