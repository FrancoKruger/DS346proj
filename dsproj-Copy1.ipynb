{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install newsapi-python\n",
    "%pip install --upgrade transformers\n",
    "%pip install bert-for-sequence-classification\n",
    "from newsapi import NewsApiClient\n",
    "import json\n",
    "import torch\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import os\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to fetch articles from API or load from local file\n",
    "def get_articles():\n",
    "    if os.path.exists('articles.json'):\n",
    "        # Load from local file if it exists\n",
    "        with open('articles.json', 'r') as json_file:\n",
    "            articles = json.load(json_file)\n",
    "            print(\"Loaded articles from local file.\")\n",
    "    else:\n",
    "        # Fetch from API if local file does not exist\n",
    "        newsapi = NewsApiClient(api_key='cc7b577d5b4b4462b6eef124170903b1')\n",
    "        articles = newsapi.get_everything(q='Trump OR Harris',\n",
    "                                          from_param='2024-09-19',\n",
    "                                          to='2024-10-01',\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=2)\n",
    "        # Save to local file\n",
    "        with open('articles.json', 'w') as json_file:\n",
    "            json.dump(articles, json_file, indent=4)\n",
    "        print(\"Articles fetched from API and saved locally.\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Call the function to get articles\n",
    "articles = get_articles()\n",
    "\n",
    "for article in articles['articles']:\n",
    "    print(article['content'], article['description'], article['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in words if word.isalpha()]\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "clean_article = [preprocess(article['content']) for article in articles['articles']]\n",
    "clean_article += [preprocess(article['description']) for article in articles['articles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Sentiment Intensity Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define lists of keywords for Trump and Kamala Harris\n",
    "trump_keywords = ['trump', 'donald trump', 'president trump']\n",
    "kamala_keywords = ['kamala', 'kamala harris', 'vice president harris']\n",
    "\n",
    "def split_sentence_by_candidate(sentence):\n",
    "    \"\"\"\n",
    "    Splits a sentence into parts mentioning Trump and Kamala based on keywords.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    trump_phrases = []\n",
    "    kamala_phrases = []\n",
    "    current_phrase = []\n",
    "    current_candidate = None\n",
    "\n",
    "    for word in words:\n",
    "        # Check if the current word refers to Trump\n",
    "        if word in trump_keywords:\n",
    "            if current_candidate == \"kamala\":\n",
    "                kamala_phrases.append(\" \".join(current_phrase))\n",
    "                current_phrase = []\n",
    "            current_candidate = \"trump\"\n",
    "        \n",
    "        # Check if the current word refers to Kamala Harris\n",
    "        elif word in kamala_keywords:\n",
    "            if current_candidate == \"trump\":\n",
    "                trump_phrases.append(\" \".join(current_phrase))\n",
    "                current_phrase = []\n",
    "            current_candidate = \"kamala\"\n",
    "        \n",
    "        current_phrase.append(word)\n",
    "\n",
    "    # Append the last phrase\n",
    "    if current_candidate == \"trump\":\n",
    "        trump_phrases.append(\" \".join(current_phrase))\n",
    "    elif current_candidate == \"kamala\":\n",
    "        kamala_phrases.append(\" \".join(current_phrase))\n",
    "    \n",
    "    return trump_phrases, kamala_phrases\n",
    "\n",
    "def calculate_sentiment_for_candidate_parts(parts):\n",
    "    \"\"\"\n",
    "    Calculate the total sentiment score for a candidate based on their parts of a sentence.\n",
    "    \"\"\"\n",
    "    total_score = 0\n",
    "    for part in parts:\n",
    "        sentiment = sid.polarity_scores(part)\n",
    "        total_score += sentiment['compound']\n",
    "    return total_score\n",
    "\n",
    "def calculate_article_sentiment(article):\n",
    "    # Tokenize article into sentences\n",
    "    sentences = sent_tokenize(article.lower())\n",
    "    \n",
    "    trump_total_score = 0\n",
    "    kamala_total_score = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Split the sentence into parts mentioning Trump and Kamala\n",
    "        trump_parts, kamala_parts = split_sentence_by_candidate(sentence)\n",
    "        \n",
    "        # Calculate sentiment for Trump and Kamala parts separately\n",
    "        trump_total_score += calculate_sentiment_for_candidate_parts(trump_parts)\n",
    "        kamala_total_score += calculate_sentiment_for_candidate_parts(kamala_parts)\n",
    "    \n",
    "    # Return final combined sentiment\n",
    "    return trump_total_score, kamala_total_score\n",
    "\n",
    "# Process each article and calculate the combined score\n",
    "articles_data =[]\n",
    "for article in clean_article:\n",
    "    trump_score, kamala_score = calculate_article_sentiment(article)\n",
    "    \n",
    "    # Calculate the final combined score (Trump score - Kamala score)\n",
    "    combined_score = trump_score - kamala_score\n",
    "    \n",
    "    print(f\"Article: {article}\")\n",
    "    print(f\"Trump Score: {trump_score}, Kamala Harris Score: {kamala_score}\")\n",
    "    print(f\"Combined Score (Trump - Kamala): {combined_score}\\n\")\n",
    "    articles_data.append({\n",
    "        'article': article,\n",
    "        'trump_score': trump_score,\n",
    "        'kamala_score': kamala_score,\n",
    "        'combined_score': trump_score - kamala_score  # Calculate the combined score\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "nrc = pd.read_csv('NRC-Emotion-Lexicon-Wordlevel-v0.92.txt', sep='\\t', names=[\"word\", \"emotion\", \"association\"])\n",
    "nrc = nrc.pivot(index='word', columns='emotion', values='association').reset_index()\n",
    "\n",
    "def get_emotions(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    emotions = {emotion: 0 for emotion in nrc.columns[1:]}\n",
    "    for word in words:\n",
    "        if word in nrc['word'].values:\n",
    "            word_emotions = nrc[nrc['word'] == word].iloc[0, 1:]\n",
    "            for emotion in word_emotions.index:\n",
    "                emotions[emotion] += word_emotions[emotion]\n",
    "    return emotions\n",
    "\n",
    "for article in clean_article:\n",
    "    emotions = get_emotions(article)\n",
    "    print(article, emotions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sentiments = ['positive', 'negative', 'neutral']\n",
    "counts = [len([s for s in clean_article if sid.polarity_scores(s)['compound'] > 0.5]),\n",
    "          len([s for s in clean_article if sid.polarity_scores(s)['compound'] < -0.5]),\n",
    "          len([s for s in clean_article if -0.5 <= sid.polarity_scores(s)['compound'] <= 0.5])]\n",
    "\n",
    "sns.barplot(x=sentiments, y=counts)\n",
    "plt.title('Sentiment Distribution for Trump and Harris News Articles')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# Add a 'combined_score' column (Trump - Kamala)\n",
    "df['combined_score'] = df['trump_score'] - df['kamala_score']\n",
    "\n",
    "# Display the DataFrame to inspect the structure\n",
    "print(df.head())\n",
    "\n",
    "# Summary statistics for sentiment scores\n",
    "print(df.describe())\n",
    "\n",
    "# Count the number of articles\n",
    "print(f\"Total number of articles: {len(df)}\")\n",
    "\n",
    "# Count articles favoring Trump (combined_score > 0) vs Kamala (combined_score < 0)\n",
    "trump_favored = (df['combined_score'] > 0).sum()\n",
    "kamala_favored = (df['combined_score'] < 0).sum()\n",
    "print(f\"Articles favoring Trump: {trump_favored}\")\n",
    "print(f\"Articles favoring Kamala: {kamala_favored}\")\n",
    "\n",
    "# Distribution of Trump and Kamala sentiment scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['trump_score'], color='blue', label='Trump Sentiment', kde=True, bins=20)\n",
    "sns.histplot(df['kamala_score'], color='red', label='Kamala Sentiment', kde=True, bins=20)\n",
    "plt.title('Sentiment Score Distribution for Trump and Kamala Harris')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Combined sentiment score (Trump - Kamala)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['combined_score'], color='green', kde=True, bins=20)\n",
    "plt.title('Combined Sentiment Score (Trump - Kamala)')\n",
    "plt.xlabel('Combined Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Function to create word cloud\n",
    "def plot_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Create word clouds for Trump and Kamala mentions\n",
    "trump_articles = \" \".join(df[df['trump_score'] != 0]['article'])\n",
    "kamala_articles = \" \".join(df[df['kamala_score'] != 0]['article'])\n",
    "\n",
    "plot_wordcloud(trump_articles, \"Word Cloud for Trump Articles\")\n",
    "plot_wordcloud(kamala_articles, \"Word Cloud for Kamala Harris Articles\")\n",
    "\n",
    "\n",
    "# Add an 'article_length' column (number of words in each article)\n",
    "df['article_length'] = df['article'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Plot sentiment scores vs article length\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='article_length', y='trump_score', data=df, label='Trump Score', color='blue')\n",
    "sns.scatterplot(x='article_length', y='kamala_score', data=df, label='Kamala Score', color='red')\n",
    "plt.title('Sentiment Scores vs Article Length')\n",
    "plt.xlabel('Article Length (Word Count)')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "most_positive_trump = df.loc[df['trump_score'].idxmax()]['article']\n",
    "most_negative_trump = df.loc[df['trump_score'].idxmin()]['article']\n",
    "most_positive_kamala = df.loc[df['kamala_score'].idxmax()]['article']\n",
    "most_negative_kamala = df.loc[df['kamala_score'].idxmin()]['article']\n",
    "\n",
    "print(f\"Most positive article for Trump: {most_positive_trump}\")\n",
    "print(f\"Most negative article for Trump: {most_negative_trump}\")\n",
    "print(f\"Most positive article for Kamala: {most_positive_kamala}\")\n",
    "print(f\"Most negative article for Kamala: {most_negative_kamala}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
